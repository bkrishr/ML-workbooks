{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkrishr/ML-workbooks/blob/main/confusion_matrix_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5iuKQRjnbbr"
      },
      "source": [
        "# Confusion Matrix and Model Evaluation\n",
        "## Complete Guide with Real-World Case Studies\n",
        "\n",
        "This notebook covers:\n",
        "1. **Confusion Matrix Fundamentals**\n",
        "2. **Metric Trade-offs** (Precision, Recall, F1-Score, Accuracy)\n",
        "3. **Real-World Case Studies**\n",
        "   - Medical Diagnosis (Cancer Detection)\n",
        "   - Fraud Detection (Credit Card)\n",
        "   - Email Spam Classification\n",
        "4. **ROC & AUC Analysis**\n",
        "5. **Balanced vs Imbalanced Dataset Strategies**\n",
        "6. **Milestone-Based Improvement Plan**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzu6nDVonbbs"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_curve, roc_auc_score, auc,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plot style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y9KDoiJnbbt"
      },
      "source": [
        "---\n",
        "## 1. Understanding Confusion Matrix\n",
        "\n",
        "### Confusion Matrix Components:\n",
        "\n",
        "```\n",
        "                 Predicted Positive    Predicted Negative\n",
        "Actual Positive      TP                     FN\n",
        "Actual Negative      FP                     TN\n",
        "```\n",
        "\n",
        "- **TP (True Positive)**: Correctly predicted positive\n",
        "- **TN (True Negative)**: Correctly predicted negative\n",
        "- **FP (False Positive)**: Type I Error - Predicted positive but actually negative\n",
        "- **FN (False Negative)**: Type II Error - Predicted negative but actually positive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1AgXcDgnbbt"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix_detailed(y_true, y_pred, title=\"Confusion Matrix\"):\n",
        "    \"\"\"\n",
        "    Plot a detailed confusion matrix with metrics\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate metrics\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Plot 1: Confusion Matrix Heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'],\n",
        "                cbar_kws={'label': 'Count'},\n",
        "                annot_kws={'size': 20, 'weight': 'bold'},\n",
        "                ax=axes[0])\n",
        "    axes[0].set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    axes[0].set_ylabel('Actual Class', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Predicted Class', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Add labels to cells\n",
        "    axes[0].text(0.5, 0.25, f'TN = {tn}', ha='center', va='center',\n",
        "                fontsize=14, color='darkgreen', weight='bold')\n",
        "    axes[0].text(1.5, 0.25, f'FP = {fp}\\n(Type I Error)', ha='center', va='center',\n",
        "                fontsize=14, color='darkred', weight='bold')\n",
        "    axes[0].text(0.5, 1.25, f'FN = {fn}\\n(Type II Error)', ha='center', va='center',\n",
        "                fontsize=14, color='darkred', weight='bold')\n",
        "    axes[0].text(1.5, 1.25, f'TP = {tp}', ha='center', va='center',\n",
        "                fontsize=14, color='darkgreen', weight='bold')\n",
        "\n",
        "    # Plot 2: Metrics Bar Chart\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "    values = [accuracy, precision, recall, f1]\n",
        "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
        "\n",
        "    bars = axes[1].bar(metrics, values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    axes[1].set_ylim([0, 1.0])\n",
        "    axes[1].set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_title('Performance Metrics', fontsize=16, fontweight='bold', pad=20)\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                    f'{value:.3f}',\n",
        "                    ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed metrics\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"DETAILED METRICS FOR: {title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\nConfusion Matrix Values:\")\n",
        "    print(f\"  True Positives (TP):  {tp:,}\")\n",
        "    print(f\"  True Negatives (TN):  {tn:,}\")\n",
        "    print(f\"  False Positives (FP): {fp:,} (Type I Error)\")\n",
        "    print(f\"  False Negatives (FN): {fn:,} (Type II Error)\")\n",
        "    print(f\"\\nPerformance Metrics:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f} = (TP + TN) / Total\")\n",
        "    print(f\"  Precision: {precision:.4f} = TP / (TP + FP)\")\n",
        "    print(f\"  Recall:    {recall:.4f} = TP / (TP + FN)\")\n",
        "    print(f\"  F1-Score:  {f1:.4f} = 2 * (Precision * Recall) / (Precision + Recall)\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    return {'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
        "            'accuracy': accuracy, 'precision': precision,\n",
        "            'recall': recall, 'f1': f1}\n",
        "\n",
        "print(\"âœ… Confusion matrix visualization function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzykFaLUnbbt"
      },
      "source": [
        "---\n",
        "## 2. Case Study 1: Medical Diagnosis (Cancer Detection)\n",
        "\n",
        "### Scenario:\n",
        "- **Problem**: Detecting cancer from medical scans\n",
        "- **Dataset**: Highly imbalanced (2% cancer prevalence)\n",
        "- **Critical**: Missing a cancer case (False Negative) can be fatal\n",
        "- **Goal**: Maximize Recall (catch all cancer cases)\n",
        "\n",
        "### Expected Behavior:\n",
        "- **High Precision Model**: Few false alarms, but misses many cancers âŒ\n",
        "- **High Recall Model**: More false alarms, but catches most cancers âœ…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AieYNthCnbbt"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic medical diagnosis data\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulate 10,000 patients with 2% cancer rate\n",
        "n_samples = 10000\n",
        "cancer_rate = 0.02\n",
        "\n",
        "# Generate features (e.g., imaging features, biomarkers)\n",
        "n_features = 20\n",
        "X_medical = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Generate labels (0 = healthy, 1 = cancer)\n",
        "y_medical = np.random.choice([0, 1], size=n_samples, p=[1-cancer_rate, cancer_rate])\n",
        "\n",
        "# Add signal to features for cancer cases\n",
        "cancer_indices = np.where(y_medical == 1)[0]\n",
        "X_medical[cancer_indices] += np.random.randn(len(cancer_indices), n_features) * 0.8\n",
        "\n",
        "# Split data\n",
        "X_train_med, X_test_med, y_train_med, y_test_med = train_test_split(\n",
        "    X_medical, y_medical, test_size=0.3, random_state=42, stratify=y_medical\n",
        ")\n",
        "\n",
        "print(\"ðŸ¥ MEDICAL DIAGNOSIS DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total patients: {n_samples:,}\")\n",
        "print(f\"Cancer cases: {np.sum(y_medical):,} ({100*cancer_rate:.1f}%)\")\n",
        "print(f\"Healthy cases: {np.sum(y_medical==0):,} ({100*(1-cancer_rate):.1f}%)\")\n",
        "print(f\"\\nTrain set: {len(X_train_med):,} samples\")\n",
        "print(f\"Test set: {len(X_test_med):,} samples\")\n",
        "print(f\"Class distribution in test: Cancer={np.sum(y_test_med):,}, Healthy={np.sum(y_test_med==0):,}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLeyLDQEnbbu"
      },
      "outputs": [],
      "source": [
        "# Train models with different threshold strategies\n",
        "\n",
        "# Model 1: High Precision (Conservative - fewer false alarms)\n",
        "model_med_precision = RandomForestClassifier(n_estimators=100, random_state=42,\n",
        "                                             class_weight={0:1, 1:5})  # Moderate weight\n",
        "model_med_precision.fit(X_train_med, y_train_med)\n",
        "y_pred_proba_prec = model_med_precision.predict_proba(X_test_med)[:, 1]\n",
        "y_pred_precision = (y_pred_proba_prec > 0.7).astype(int)  # High threshold = High precision\n",
        "\n",
        "# Model 2: High Recall (Aggressive - catch all cancers)\n",
        "model_med_recall = RandomForestClassifier(n_estimators=100, random_state=42,\n",
        "                                         class_weight={0:1, 1:20})  # Heavy weight on minority\n",
        "model_med_recall.fit(X_train_med, y_train_med)\n",
        "y_pred_proba_rec = model_med_recall.predict_proba(X_test_med)[:, 1]\n",
        "y_pred_recall = (y_pred_proba_rec > 0.2).astype(int)  # Low threshold = High recall\n",
        "\n",
        "print(\"âœ… Medical diagnosis models trained!\")\n",
        "print(\"\\nModel 1: High Precision Strategy (Threshold = 0.7)\")\n",
        "print(\"Model 2: High Recall Strategy (Threshold = 0.2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYvn5J9Knbbu"
      },
      "outputs": [],
      "source": [
        "# Evaluate High Precision Model\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# SCENARIO 1: HIGH PRECISION MODEL (Conservative)\")\n",
        "print(\"#\"*70)\n",
        "metrics_prec = plot_confusion_matrix_detailed(\n",
        "    y_test_med, y_pred_precision,\n",
        "    \"Medical Diagnosis: High Precision Model\"\n",
        ")\n",
        "\n",
        "print(\"\\nâš ï¸  ANALYSIS:\")\n",
        "print(f\"   - Only {metrics_prec['fp']} false alarms (good for patient stress)\")\n",
        "print(f\"   - BUT MISSED {metrics_prec['fn']} CANCER CASES (DANGEROUS!)\")\n",
        "print(f\"   - Recall of {metrics_prec['recall']:.1%} means we miss {100*(1-metrics_prec['recall']):.1f}% of cancers\")\n",
        "print(f\"   - This is UNACCEPTABLE in medical diagnosis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89HWbmmXnbbu"
      },
      "outputs": [],
      "source": [
        "# Evaluate High Recall Model\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# SCENARIO 2: HIGH RECALL MODEL (Aggressive)\")\n",
        "print(\"#\"*70)\n",
        "metrics_rec = plot_confusion_matrix_detailed(\n",
        "    y_test_med, y_pred_recall,\n",
        "    \"Medical Diagnosis: High Recall Model\"\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… ANALYSIS:\")\n",
        "print(f\"   - {metrics_rec['fp']} false alarms (more stress, but manageable with follow-up tests)\")\n",
        "print(f\"   - Only missed {metrics_rec['fn']} cancer cases (much better!)\")\n",
        "print(f\"   - Recall of {metrics_rec['recall']:.1%} means we catch {100*metrics_rec['recall']:.1f}% of cancers\")\n",
        "print(f\"   - This is PREFERRED in medical diagnosis - better safe than sorry!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzXMQUGFnbbu"
      },
      "outputs": [],
      "source": [
        "# Compare the models side by side\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'False Negatives', 'False Positives'],\n",
        "    'High Precision\\n(Conservative)': [\n",
        "        f\"{metrics_prec['accuracy']:.3f}\",\n",
        "        f\"{metrics_prec['precision']:.3f}\",\n",
        "        f\"{metrics_prec['recall']:.3f} âš ï¸\",\n",
        "        f\"{metrics_prec['f1']:.3f}\",\n",
        "        f\"{metrics_prec['fn']} ðŸš¨\",\n",
        "        f\"{metrics_prec['fp']}\"\n",
        "    ],\n",
        "    'High Recall\\n(Aggressive)': [\n",
        "        f\"{metrics_rec['accuracy']:.3f}\",\n",
        "        f\"{metrics_rec['precision']:.3f}\",\n",
        "        f\"{metrics_rec['recall']:.3f} âœ…\",\n",
        "        f\"{metrics_rec['f1']:.3f}\",\n",
        "        f\"{metrics_rec['fn']} âœ…\",\n",
        "        f\"{metrics_rec['fp']}\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MEDICAL DIAGNOSIS: MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "print(\"\\nðŸŽ¯ KEY TAKEAWAY FOR MEDICAL DIAGNOSIS:\")\n",
        "print(\"   â†’ Prioritize RECALL over Precision\")\n",
        "print(\"   â†’ Missing a cancer case (FN) is far worse than a false alarm (FP)\")\n",
        "print(\"   â†’ False alarms can be resolved with follow-up tests\")\n",
        "print(\"   â†’ Missed cancer can be fatal\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPjzs6Z3nbbu"
      },
      "source": [
        "---\n",
        "## 3. Case Study 2: Fraud Detection (Credit Card)\n",
        "\n",
        "### Scenario:\n",
        "- **Problem**: Detecting fraudulent credit card transactions\n",
        "- **Dataset**: Extremely imbalanced (0.1% fraud rate)\n",
        "- **Trade-off**:\n",
        "  - High Recall: Catch fraud but block legitimate transactions (bad UX)\n",
        "  - High Precision: Few false blocks but miss some fraud (financial loss)\n",
        "- **Goal**: Balance between catching fraud and customer experience (F1-Score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCAHPqzonbbu"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic fraud detection data\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulate 100,000 transactions with 0.1% fraud rate\n",
        "n_transactions = 100000\n",
        "fraud_rate = 0.001\n",
        "\n",
        "# Generate features (transaction amount, time, location features, etc.)\n",
        "n_features_fraud = 25\n",
        "X_fraud = np.random.randn(n_transactions, n_features_fraud)\n",
        "\n",
        "# Generate labels (0 = legitimate, 1 = fraud)\n",
        "y_fraud = np.random.choice([0, 1], size=n_transactions, p=[1-fraud_rate, fraud_rate])\n",
        "\n",
        "# Add strong signal to fraudulent transactions\n",
        "fraud_indices = np.where(y_fraud == 1)[0]\n",
        "X_fraud[fraud_indices] += np.random.randn(len(fraud_indices), n_features_fraud) * 1.5\n",
        "\n",
        "# Split data\n",
        "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
        "    X_fraud, y_fraud, test_size=0.3, random_state=42, stratify=y_fraud\n",
        ")\n",
        "\n",
        "print(\"ðŸ’³ CREDIT CARD FRAUD DETECTION DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total transactions: {n_transactions:,}\")\n",
        "print(f\"Fraudulent: {np.sum(y_fraud):,} ({100*fraud_rate:.2f}%)\")\n",
        "print(f\"Legitimate: {np.sum(y_fraud==0):,} ({100*(1-fraud_rate):.2f}%)\")\n",
        "print(f\"\\nTrain set: {len(X_train_fraud):,} transactions\")\n",
        "print(f\"Test set: {len(X_test_fraud):,} transactions\")\n",
        "print(f\"Fraud in test: {np.sum(y_test_fraud):,} cases\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etIbghjRnbbu"
      },
      "outputs": [],
      "source": [
        "# Use SMOTE to handle extreme imbalance\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.1)\n",
        "X_train_fraud_balanced, y_train_fraud_balanced = smote.fit_resample(X_train_fraud, y_train_fraud)\n",
        "\n",
        "print(f\"Original training data: {len(y_train_fraud):,} samples\")\n",
        "print(f\"After SMOTE: {len(y_train_fraud_balanced):,} samples\")\n",
        "print(f\"Fraud cases after SMOTE: {np.sum(y_train_fraud_balanced):,}\")\n",
        "\n",
        "# Model 1: High Precision (Conservative)\n",
        "model_fraud_prec = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
        "model_fraud_prec.fit(X_train_fraud_balanced, y_train_fraud_balanced)\n",
        "y_pred_proba_fraud_prec = model_fraud_prec.predict_proba(X_test_fraud)[:, 1]\n",
        "y_pred_fraud_prec = (y_pred_proba_fraud_prec > 0.8).astype(int)  # High threshold\n",
        "\n",
        "# Model 2: Balanced F1 (Optimal)\n",
        "model_fraud_balanced = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
        "model_fraud_balanced.fit(X_train_fraud_balanced, y_train_fraud_balanced)\n",
        "y_pred_proba_fraud_bal = model_fraud_balanced.predict_proba(X_test_fraud)[:, 1]\n",
        "y_pred_fraud_bal = (y_pred_proba_fraud_bal > 0.3).astype(int)  # Optimized threshold\n",
        "\n",
        "print(\"\\nâœ… Fraud detection models trained!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DVd-ICinbbu"
      },
      "outputs": [],
      "source": [
        "# Evaluate High Precision Model\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# SCENARIO 1: HIGH PRECISION MODEL (Conservative)\")\n",
        "print(\"#\"*70)\n",
        "metrics_fraud_prec = plot_confusion_matrix_detailed(\n",
        "    y_test_fraud, y_pred_fraud_prec,\n",
        "    \"Fraud Detection: High Precision Model\"\n",
        ")\n",
        "\n",
        "print(\"\\nâš ï¸  ANALYSIS:\")\n",
        "print(f\"   - Very few false blocks: {metrics_fraud_prec['fp']} legitimate transactions blocked\")\n",
        "print(f\"   - BUT missed {metrics_fraud_prec['fn']} fraudulent transactions (financial loss!)\")\n",
        "print(f\"   - Good customer experience, but significant fraud gets through\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEHSqafBnbbu"
      },
      "outputs": [],
      "source": [
        "# Evaluate Balanced Model\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# SCENARIO 2: BALANCED F1 MODEL (Optimal)\")\n",
        "print(\"#\"*70)\n",
        "metrics_fraud_bal = plot_confusion_matrix_detailed(\n",
        "    y_test_fraud, y_pred_fraud_bal,\n",
        "    \"Fraud Detection: Balanced F1 Model\"\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… ANALYSIS:\")\n",
        "print(f\"   - {metrics_fraud_bal['fp']} false blocks (acceptable with good customer communication)\")\n",
        "print(f\"   - Only {metrics_fraud_bal['fn']} frauds missed (much better protection!)\")\n",
        "print(f\"   - Good balance: {100*metrics_fraud_bal['recall']:.1f}% fraud caught, \"\n",
        "      f\"{100*metrics_fraud_bal['precision']:.1f}% accuracy when flagging\")\n",
        "print(f\"   - F1-Score of {metrics_fraud_bal['f1']:.3f} indicates good balance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_lofWtjnbbu"
      },
      "outputs": [],
      "source": [
        "# Financial impact analysis\n",
        "avg_fraud_amount = 150  # Average fraud transaction $150\n",
        "avg_legit_amount = 75   # Average legitimate transaction $75\n",
        "\n",
        "# High Precision Model costs\n",
        "cost_missed_fraud_prec = metrics_fraud_prec['fn'] * avg_fraud_amount\n",
        "cost_false_blocks_prec = metrics_fraud_prec['fp'] * 5  # $5 customer service cost per false block\n",
        "total_cost_prec = cost_missed_fraud_prec + cost_false_blocks_prec\n",
        "\n",
        "# Balanced Model costs\n",
        "cost_missed_fraud_bal = metrics_fraud_bal['fn'] * avg_fraud_amount\n",
        "cost_false_blocks_bal = metrics_fraud_bal['fp'] * 5\n",
        "total_cost_bal = cost_missed_fraud_bal + cost_false_blocks_bal\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FRAUD DETECTION: FINANCIAL IMPACT ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nHigh Precision Model:\")\n",
        "print(f\"  Cost of missed fraud: ${cost_missed_fraud_prec:,.2f}\")\n",
        "print(f\"  Cost of false blocks: ${cost_false_blocks_prec:,.2f}\")\n",
        "print(f\"  TOTAL COST: ${total_cost_prec:,.2f}\")\n",
        "print(\"\\nBalanced F1 Model:\")\n",
        "print(f\"  Cost of missed fraud: ${cost_missed_fraud_bal:,.2f}\")\n",
        "print(f\"  Cost of false blocks: ${cost_false_blocks_bal:,.2f}\")\n",
        "print(f\"  TOTAL COST: ${total_cost_bal:,.2f}\")\n",
        "print(f\"\\nðŸ’° SAVINGS with Balanced Model: ${total_cost_prec - total_cost_bal:,.2f}\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nðŸŽ¯ KEY TAKEAWAY FOR FRAUD DETECTION:\")\n",
        "print(\"   â†’ Use F1-Score to balance fraud detection and customer experience\")\n",
        "print(\"   â†’ Both Precision and Recall matter\")\n",
        "print(\"   â†’ Optimize for total cost (fraud loss + customer friction)\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Dma3DHnbbu"
      },
      "source": [
        "---\n",
        "## 4. Case Study 3: Email Spam Classification\n",
        "\n",
        "### Scenario:\n",
        "- **Problem**: Classifying emails as spam or legitimate\n",
        "- **Dataset**: Moderately balanced (40% spam)\n",
        "- **Critical**: Blocking important emails (FP) is worse than letting spam through (FN)\n",
        "- **Goal**: Maximize Precision (minimize false spam classifications)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ75Kocxnbbu"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic email spam data\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulate 20,000 emails with 40% spam rate\n",
        "n_emails = 20000\n",
        "spam_rate = 0.4\n",
        "\n",
        "# Generate features (word counts, sender features, etc.)\n",
        "n_features_email = 30\n",
        "X_email = np.random.randn(n_emails, n_features_email)\n",
        "\n",
        "# Generate labels (0 = legitimate, 1 = spam)\n",
        "y_email = np.random.choice([0, 1], size=n_emails, p=[1-spam_rate, spam_rate])\n",
        "\n",
        "# Add signal to spam emails\n",
        "spam_indices = np.where(y_email == 1)[0]\n",
        "X_email[spam_indices] += np.random.randn(len(spam_indices), n_features_email) * 1.0\n",
        "\n",
        "# Split data\n",
        "X_train_email, X_test_email, y_train_email, y_test_email = train_test_split(\n",
        "    X_email, y_email, test_size=0.3, random_state=42, stratify=y_email\n",
        ")\n",
        "\n",
        "print(\"ðŸ“§ EMAIL SPAM CLASSIFICATION DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total emails: {n_emails:,}\")\n",
        "print(f\"Spam emails: {np.sum(y_email):,} ({100*spam_rate:.1f}%)\")\n",
        "print(f\"Legitimate emails: {np.sum(y_email==0):,} ({100*(1-spam_rate):.1f}%)\")\n",
        "print(f\"\\nTrain set: {len(X_train_email):,} emails\")\n",
        "print(f\"Test set: {len(X_test_email):,} emails\")\n",
        "print(\"\\nâš–ï¸ This is a MODERATELY BALANCED dataset\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPjOtlKxnbbv"
      },
      "outputs": [],
      "source": [
        "# Model 1: High Recall (Aggressive spam filter)\n",
        "model_email_recall = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_email_recall.fit(X_train_email, y_train_email)\n",
        "y_pred_proba_email_rec = model_email_recall.predict_proba(X_test_email)[:, 1]\n",
        "y_pred_email_rec = (y_pred_proba_email_rec > 0.3).astype(int)  # Low threshold = High recall\n",
        "\n",
        "# Model 2: High Precision (Conservative spam filter)\n",
        "model_email_precision = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_email_precision.fit(X_train_email, y_train_email)\n",
        "y_pred_proba_email_prec = model_email_precision.predict_proba(X_test_email)[:, 1]\n",
        "y_pred_email_prec = (y_pred_proba_email_prec > 0.7).astype(int)  # High threshold = High precision\n",
        "\n",
        "print(\"âœ… Email spam classification models trained!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPsktTgUnbbv"
      },
      "outputs": [],
      "source": [
        "# Evaluate High Recall Model\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# SCENARIO 1: HIGH RECALL MODEL (Aggressive Spam Filter)\")\n",
        "print(\"#\"*70)\n",
        "metrics_email_rec = plot_confusion_matrix_detailed(\n",
        "    y_test_email, y_pred_email_rec,\n",
        "    \"Email Spam: High Recall Model\"\n",
        ")\n",
        "\n",
        "print(\"\\nâŒ ANALYSIS:\")\n",
        "print(f\"   - Catches most spam: only {metrics_email_rec['fn']} spam emails get through\")\n",
        "print(f\"   - BUT {metrics_email_rec['fp']} IMPORTANT EMAILS BLOCKED (Bad user experience!)\")\n",
        "print(f\"   - Users miss important messages from work, family, bills, etc.\")\n",
        "print(f\"   - This creates frustration and distrust in the spam filter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb_K9meVnbbv"
      },
      "outputs": [],
      "source": [
        "# Evaluate High Precision Model\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# SCENARIO 2: HIGH PRECISION MODEL (Conservative Spam Filter)\")\n",
        "print(\"#\"*70)\n",
        "metrics_email_prec = plot_confusion_matrix_detailed(\n",
        "    y_test_email, y_pred_email_prec,\n",
        "    \"Email Spam: High Precision Model\"\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… ANALYSIS:\")\n",
        "print(f\"   - Only {metrics_email_prec['fp']} legitimate emails wrongly blocked (excellent!)\")\n",
        "print(f\"   - {metrics_email_prec['fn']} spam emails get through (annoying but not critical)\")\n",
        "print(f\"   - {100*metrics_email_prec['precision']:.1f}% of spam-flagged emails are actually spam\")\n",
        "print(f\"   - Users can manually delete spam, but can't recover missed important emails\")\n",
        "print(f\"   - This is PREFERRED for email - better to see some spam than miss important mail\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVcMsQRLnbbv"
      },
      "outputs": [],
      "source": [
        "# User experience impact\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EMAIL SPAM: USER EXPERIENCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nHigh Recall Model (Aggressive):\")\n",
        "print(f\"  âœ… Spam caught: {metrics_email_rec['tp']} / {metrics_email_rec['tp'] + metrics_email_rec['fn']} \"\n",
        "      f\"({100*metrics_email_rec['recall']:.1f}%)\")\n",
        "print(f\"  âŒ Important emails blocked: {metrics_email_rec['fp']} (UNACCEPTABLE)\")\n",
        "print(f\"  ðŸ˜  User frustration: HIGH (missing important emails)\")\n",
        "\n",
        "print(\"\\nHigh Precision Model (Conservative):\")\n",
        "print(f\"  âœ… Important emails safe: {metrics_email_prec['tn']} / {metrics_email_prec['tn'] + metrics_email_prec['fp']} \"\n",
        "      f\"({100*metrics_email_prec['tn']/(metrics_email_prec['tn'] + metrics_email_prec['fp']):.1f}%)\")\n",
        "print(f\"  ðŸ“§ Spam in inbox: {metrics_email_prec['fn']} (annoying but manageable)\")\n",
        "print(f\"  ðŸ˜Š User satisfaction: HIGH (no important emails lost)\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\\nðŸŽ¯ KEY TAKEAWAY FOR EMAIL SPAM:\")\n",
        "print(\"   â†’ Prioritize PRECISION over Recall\")\n",
        "print(\"   â†’ False Positive (blocking important email) is worse than False Negative (spam in inbox)\")\n",
        "print(\"   â†’ Users can delete spam, but can't recover blocked important emails\")\n",
        "print(\"   â†’ Trust and user experience are paramount\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nrVeua1nbbv"
      },
      "source": [
        "---\n",
        "## 5. ROC Curve and AUC Analysis\n",
        "\n",
        "### ROC Curve (Receiver Operating Characteristic):\n",
        "- Plots **True Positive Rate (TPR)** vs **False Positive Rate (FPR)**\n",
        "- TPR = Recall = TP / (TP + FN)\n",
        "- FPR = FP / (FP + TN)\n",
        "- Shows model performance across all classification thresholds\n",
        "\n",
        "### AUC (Area Under Curve):\n",
        "- Single metric summarizing ROC curve\n",
        "- Range: 0 to 1\n",
        "- Interpretation:\n",
        "  - 0.90-1.00: Excellent\n",
        "  - 0.80-0.90: Good\n",
        "  - 0.70-0.80: Fair\n",
        "  - 0.60-0.70: Poor\n",
        "  - 0.50: Random (no skill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxpHv9nnnbbv"
      },
      "outputs": [],
      "source": [
        "def plot_roc_comparison(models_dict, X_test, y_test, title=\"ROC Curve Comparison\"):\n",
        "    \"\"\"\n",
        "    Plot ROC curves for multiple models\n",
        "\n",
        "    models_dict: {'Model Name': (model, color)}\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Create subplots\n",
        "    gs = plt.GridSpec(2, 2, hspace=0.3, wspace=0.3)\n",
        "    ax1 = plt.subplot(gs[0, :])\n",
        "    ax2 = plt.subplot(gs[1, 0])\n",
        "    ax3 = plt.subplot(gs[1, 1])\n",
        "\n",
        "    # Plot 1: ROC Curves\n",
        "    for name, (model, color) in models_dict.items():\n",
        "        # Get prediction probabilities\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_pred_proba = model.decision_function(X_test)\n",
        "\n",
        "        # Calculate ROC curve\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot ROC curve\n",
        "        ax1.plot(fpr, tpr, color=color, lw=3,\n",
        "                label=f'{name} (AUC = {roc_auc:.3f})', alpha=0.8)\n",
        "\n",
        "    # Plot diagonal (random classifier)\n",
        "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)', alpha=0.5)\n",
        "\n",
        "    ax1.set_xlim([-0.02, 1.02])\n",
        "    ax1.set_ylim([-0.02, 1.02])\n",
        "    ax1.set_xlabel('False Positive Rate (FPR)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('True Positive Rate (TPR) / Recall', fontsize=14, fontweight='bold')\n",
        "    ax1.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    ax1.legend(loc='lower right', fontsize=12, framealpha=0.9)\n",
        "    ax1.grid(alpha=0.3)\n",
        "\n",
        "    # Add annotations\n",
        "    ax1.annotate('Perfect Classifier\\n(TPR=1, FPR=0)', xy=(0, 1), xytext=(0.3, 0.85),\n",
        "                fontsize=11, ha='center',\n",
        "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.7),\n",
        "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3', lw=2))\n",
        "\n",
        "    # Plot 2: AUC Bar Chart\n",
        "    auc_scores = []\n",
        "    model_names = []\n",
        "    colors_list = []\n",
        "\n",
        "    for name, (model, color) in models_dict.items():\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_pred_proba = model.decision_function(X_test)\n",
        "\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        auc_scores.append(roc_auc)\n",
        "        model_names.append(name)\n",
        "        colors_list.append(color)\n",
        "\n",
        "    bars = ax2.barh(model_names, auc_scores, color=colors_list, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    ax2.set_xlim([0.5, 1.0])\n",
        "    ax2.set_xlabel('AUC Score', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('AUC Comparison', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for bar, score in zip(bars, auc_scores):\n",
        "        width = bar.get_width()\n",
        "        ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
        "                f'{score:.4f}',\n",
        "                ha='left', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "    # Add AUC interpretation zones\n",
        "    ax2.axvspan(0.90, 1.0, alpha=0.2, color='green', label='Excellent')\n",
        "    ax2.axvspan(0.80, 0.90, alpha=0.2, color='lightgreen', label='Good')\n",
        "    ax2.axvspan(0.70, 0.80, alpha=0.2, color='yellow', label='Fair')\n",
        "    ax2.axvspan(0.50, 0.70, alpha=0.2, color='orange', label='Poor')\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    for name, (model, color) in models_dict.items():\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_pred_proba = model.decision_function(X_test)\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "        avg_precision = average_precision_score(y_test, y_pred_proba)\n",
        "\n",
        "        ax3.plot(recall, precision, color=color, lw=3,\n",
        "                label=f'{name} (AP = {avg_precision:.3f})', alpha=0.8)\n",
        "\n",
        "    ax3.set_xlim([-0.02, 1.02])\n",
        "    ax3.set_ylim([-0.02, 1.02])\n",
        "    ax3.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
        "    ax3.set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
        "    ax3.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "    ax3.legend(loc='best', fontsize=10)\n",
        "    ax3.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"âœ… ROC and PR curve plotting function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzIHWfMenbbv"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curves for Medical Diagnosis models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROC & AUC ANALYSIS: MEDICAL DIAGNOSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "medical_models = {\n",
        "    'High Precision Model': (model_med_precision, '#e74c3c'),\n",
        "    'High Recall Model': (model_med_recall, '#2ecc71')\n",
        "}\n",
        "\n",
        "plot_roc_comparison(medical_models, X_test_med, y_test_med,\n",
        "                   \"ROC Curve: Medical Diagnosis Models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2bZytfUnbbv"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curves for Fraud Detection models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROC & AUC ANALYSIS: FRAUD DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fraud_models = {\n",
        "    'High Precision Model': (model_fraud_prec, '#3498db'),\n",
        "    'Balanced F1 Model': (model_fraud_balanced, '#f39c12')\n",
        "}\n",
        "\n",
        "plot_roc_comparison(fraud_models, X_test_fraud, y_test_fraud,\n",
        "                   \"ROC Curve: Fraud Detection Models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezz609kSnbbv"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curves for Email Spam models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROC & AUC ANALYSIS: EMAIL SPAM CLASSIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "email_models = {\n",
        "    'High Recall Model': (model_email_recall, '#9b59b6'),\n",
        "    'High Precision Model': (model_email_precision, '#1abc9c')\n",
        "}\n",
        "\n",
        "plot_roc_comparison(email_models, X_test_email, y_test_email,\n",
        "                   \"ROC Curve: Email Spam Classification Models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cWXs0tQnbbv"
      },
      "source": [
        "---\n",
        "## 6. Threshold Optimization\n",
        "\n",
        "Demonstration of how changing classification threshold affects metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cx_we8jnbbv"
      },
      "outputs": [],
      "source": [
        "def plot_threshold_analysis(model, X_test, y_test, title=\"Threshold Analysis\"):\n",
        "    \"\"\"\n",
        "    Plot how metrics change with different classification thresholds\n",
        "    \"\"\"\n",
        "    # Get prediction probabilities\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Test different thresholds\n",
        "    thresholds = np.linspace(0.1, 0.9, 50)\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "        if len(np.unique(y_pred)) > 1:  # At least one of each class predicted\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "            precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
        "            recalls.append(recall_score(y_test, y_pred, zero_division=0))\n",
        "            f1_scores.append(f1_score(y_test, y_pred, zero_division=0))\n",
        "        else:\n",
        "            accuracies.append(np.nan)\n",
        "            precisions.append(np.nan)\n",
        "            recalls.append(np.nan)\n",
        "            f1_scores.append(np.nan)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    plt.plot(thresholds, accuracies, 'o-', label='Accuracy', linewidth=2, markersize=4, color='#3498db')\n",
        "    plt.plot(thresholds, precisions, 's-', label='Precision', linewidth=2, markersize=4, color='#e74c3c')\n",
        "    plt.plot(thresholds, recalls, '^-', label='Recall', linewidth=2, markersize=4, color='#2ecc71')\n",
        "    plt.plot(thresholds, f1_scores, 'd-', label='F1-Score', linewidth=2, markersize=4, color='#f39c12')\n",
        "\n",
        "    plt.xlabel('Classification Threshold', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Score', fontsize=14, fontweight='bold')\n",
        "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.legend(fontsize=12, loc='best')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.ylim([0, 1.05])\n",
        "\n",
        "    # Annotate key points\n",
        "    # Find optimal F1 threshold\n",
        "    if not all(np.isnan(f1_scores)):\n",
        "        optimal_idx = np.nanargmax(f1_scores)\n",
        "        optimal_threshold = thresholds[optimal_idx]\n",
        "        optimal_f1 = f1_scores[optimal_idx]\n",
        "\n",
        "        plt.axvline(x=optimal_threshold, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
        "        plt.annotate(f'Optimal F1\\nThreshold: {optimal_threshold:.2f}\\nF1: {optimal_f1:.3f}',\n",
        "                    xy=(optimal_threshold, optimal_f1), xytext=(optimal_threshold + 0.15, optimal_f1 - 0.15),\n",
        "                    fontsize=11, ha='left',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
        "                    arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print insights\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"THRESHOLD ANALYSIS INSIGHTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nðŸ“Š As threshold increases (0.1 â†’ 0.9):\")\n",
        "    print(f\"   â€¢ Precision typically INCREASES (fewer false positives)\")\n",
        "    print(f\"   â€¢ Recall typically DECREASES (more false negatives)\")\n",
        "    print(f\"   â€¢ Accuracy depends on class balance\")\n",
        "    print(f\"   â€¢ F1-Score shows the optimal balance point\")\n",
        "    print(f\"\\nðŸŽ¯ Choose threshold based on business requirements:\")\n",
        "    print(f\"   â€¢ Low threshold (0.2-0.4): High Recall, lower Precision\")\n",
        "    print(f\"   â€¢ Medium threshold (0.4-0.6): Balanced F1\")\n",
        "    print(f\"   â€¢ High threshold (0.6-0.8): High Precision, lower Recall\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "print(\"âœ… Threshold analysis function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2TyDI8Anbbv"
      },
      "outputs": [],
      "source": [
        "# Analyze threshold impact for Fraud Detection\n",
        "plot_threshold_analysis(model_fraud_balanced, X_test_fraud, y_test_fraud,\n",
        "                       \"Threshold Analysis: Fraud Detection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHUpe68-nbbv"
      },
      "source": [
        "---\n",
        "## 7. Balanced vs Imbalanced Dataset Strategy\n",
        "\n",
        "### Summary of Metric Selection\n",
        "\n",
        "| Dataset Type | Primary Metrics | Why |\n",
        "|-------------|-----------------|-----|\n",
        "| **Balanced** | Accuracy, F1-Score, ROC-AUC | All classes well represented |\n",
        "| **Imbalanced** | PR-AUC, F1-Score, Recall/Precision | Accuracy is misleading |\n",
        "\n",
        "### When to Use Each Metric:\n",
        "\n",
        "1. **Accuracy**: Balanced datasets, overall correctness matters\n",
        "2. **Precision**: When False Positives are costly (spam filter, expensive interventions)\n",
        "3. **Recall**: When False Negatives are costly (disease detection, fraud)\n",
        "4. **F1-Score**: When you need balance between Precision and Recall\n",
        "5. **ROC-AUC**: Comparing models, balanced datasets\n",
        "6. **PR-AUC**: Imbalanced datasets (better than ROC-AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b38WpCknbbv"
      },
      "outputs": [],
      "source": [
        "# Create comparison visualization\n",
        "def create_metric_selection_guide():\n",
        "    \"\"\"\n",
        "    Visual guide for metric selection\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Metric Selection Decision Guide', fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "    # Plot 1: Dataset Balance Decision\n",
        "    ax1 = axes[0, 0]\n",
        "    balance_categories = ['Balanced\\n(40-60% each)', 'Slightly Imbalanced\\n(20-40% minority)',\n",
        "                         'Imbalanced\\n(5-20% minority)', 'Highly Imbalanced\\n(<5% minority)']\n",
        "    recommended_metrics = ['Accuracy\\nF1-Score', 'F1-Score\\nRecall',\n",
        "                          'Recall\\nPR-AUC', 'Recall\\nPR-AUC']\n",
        "    colors_balance = ['#2ecc71', '#f39c12', '#e74c3c', '#c0392b']\n",
        "\n",
        "    y_pos = np.arange(len(balance_categories))\n",
        "    bars = ax1.barh(y_pos, [1]*len(balance_categories), color=colors_balance, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    ax1.set_yticks(y_pos)\n",
        "    ax1.set_yticklabels(balance_categories, fontsize=11)\n",
        "    ax1.set_xlabel('Dataset Balance Level', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title('Step 1: Assess Dataset Balance', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlim([0, 1.5])\n",
        "    ax1.set_xticks([])\n",
        "\n",
        "    for i, (bar, metric) in enumerate(zip(bars, recommended_metrics)):\n",
        "        ax1.text(0.5, i, metric, ha='center', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "    # Plot 2: Cost of Errors Decision\n",
        "    ax2 = axes[0, 1]\n",
        "    error_types = ['FP > FN\\n(Spam Filter)', 'FN > FP\\n(Disease Detection)',\n",
        "                  'FP â‰ˆ FN\\n(General Classification)', 'Both Critical\\n(Safety Systems)']\n",
        "    metric_choice = ['Precision', 'Recall', 'F1-Score', 'Maximize\\nBoth']\n",
        "    colors_error = ['#3498db', '#e74c3c', '#f39c12', '#9b59b6']\n",
        "\n",
        "    bars2 = ax2.barh(np.arange(len(error_types)), [1]*len(error_types),\n",
        "                     color=colors_error, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    ax2.set_yticks(np.arange(len(error_types)))\n",
        "    ax2.set_yticklabels(error_types, fontsize=11)\n",
        "    ax2.set_xlabel('Error Cost Scenario', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Step 2: Evaluate Error Costs', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlim([0, 1.5])\n",
        "    ax2.set_xticks([])\n",
        "\n",
        "    for i, (bar, metric) in enumerate(zip(bars2, metric_choice)):\n",
        "        ax2.text(0.5, i, metric, ha='center', va='center', fontsize=12, fontweight='bold', color='white')\n",
        "\n",
        "    # Plot 3: Use Case Examples\n",
        "    ax3 = axes[1, 0]\n",
        "    use_cases = [\n",
        "        ('Medical\\nDiagnosis', 'Recall', '#e74c3c'),\n",
        "        ('Fraud\\nDetection', 'F1-Score', '#f39c12'),\n",
        "        ('Spam\\nFilter', 'Precision', '#3498db'),\n",
        "        ('Credit\\nScoring', 'PR-AUC', '#9b59b6'),\n",
        "        ('Quality\\nControl', 'Recall', '#e74c3c')\n",
        "    ]\n",
        "\n",
        "    use_case_names = [uc[0] for uc in use_cases]\n",
        "    use_case_metrics = [uc[1] for uc in use_cases]\n",
        "    use_case_colors = [uc[2] for uc in use_cases]\n",
        "\n",
        "    bars3 = ax3.bar(range(len(use_cases)), [1]*len(use_cases),\n",
        "                    color=use_case_colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    ax3.set_xticks(range(len(use_cases)))\n",
        "    ax3.set_xticklabels(use_case_names, fontsize=11, rotation=0)\n",
        "    ax3.set_ylabel('Priority Metric', fontsize=12, fontweight='bold')\n",
        "    ax3.set_title('Step 3: Real-World Use Cases', fontsize=14, fontweight='bold')\n",
        "    ax3.set_ylim([0, 1.3])\n",
        "    ax3.set_yticks([])\n",
        "\n",
        "    for i, (bar, metric) in enumerate(zip(bars3, use_case_metrics)):\n",
        "        ax3.text(i, 0.5, metric, ha='center', va='center', fontsize=11, fontweight='bold', color='white')\n",
        "\n",
        "    # Plot 4: Metric Trade-offs\n",
        "    ax4 = axes[1, 1]\n",
        "\n",
        "    tradeoff_text = [\n",
        "        \"Precision â†‘ â†’ Recall â†“\",\n",
        "        \"Recall â†‘ â†’ Precision â†“\",\n",
        "        \"F1 balances both\",\n",
        "        \"Accuracy misleads on\\nimbalanced data\",\n",
        "        \"ROC-AUC: balanced data\",\n",
        "        \"PR-AUC: imbalanced data\"\n",
        "    ]\n",
        "\n",
        "    ax4.axis('off')\n",
        "    ax4.set_title('Step 4: Understand Trade-offs', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "    y_text = 0.9\n",
        "    for i, text in enumerate(tradeoff_text):\n",
        "        color = ['#e74c3c', '#2ecc71', '#f39c12', '#c0392b', '#3498db', '#9b59b6'][i]\n",
        "        ax4.text(0.5, y_text, text, ha='center', va='center', fontsize=13,\n",
        "                fontweight='bold',\n",
        "                bbox=dict(boxstyle='round,pad=0.8', facecolor=color, alpha=0.3, edgecolor=color, linewidth=2))\n",
        "        y_text -= 0.15\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "create_metric_selection_guide()\n",
        "print(\"\\nâœ… Metric selection guide created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG2qYJ8Inbbw"
      },
      "source": [
        "---\n",
        "## 8. Milestone-Based Model Improvement Strategy\n",
        "\n",
        "Step-by-step approach to improving model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKr5ny-Xnbbw"
      },
      "outputs": [],
      "source": [
        "# Demonstrate milestone-based improvement on fraud detection\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MILESTONE-BASED IMPROVEMENT: FRAUD DETECTION EXAMPLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Baseline Model (Phase 1)\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"# PHASE 1: BASELINE MODEL\")\n",
        "print(\"#\"*80)\n",
        "baseline_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "baseline_model.fit(X_train_fraud, y_train_fraud)\n",
        "y_pred_baseline = baseline_model.predict(X_test_fraud)\n",
        "\n",
        "metrics_baseline = plot_confusion_matrix_detailed(\n",
        "    y_test_fraud, y_pred_baseline,\n",
        "    \"Phase 1: Baseline Logistic Regression\"\n",
        ")\n",
        "\n",
        "baseline_f1 = metrics_baseline['f1']\n",
        "baseline_recall = metrics_baseline['recall']\n",
        "print(f\"\\nðŸ“Š Baseline Metrics: F1={baseline_f1:.3f}, Recall={baseline_recall:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8vgEI3inbbw"
      },
      "outputs": [],
      "source": [
        "# Phase 2: Threshold Optimization\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"# PHASE 2: THRESHOLD OPTIMIZATION\")\n",
        "print(\"#\"*80)\n",
        "\n",
        "y_pred_proba_baseline = baseline_model.predict_proba(X_test_fraud)[:, 1]\n",
        "\n",
        "# Find optimal threshold for F1\n",
        "best_f1 = 0\n",
        "best_threshold = 0.5\n",
        "for threshold in np.linspace(0.1, 0.9, 50):\n",
        "    y_pred_thresh = (y_pred_proba_baseline >= threshold).astype(int)\n",
        "    if len(np.unique(y_pred_thresh)) > 1:\n",
        "        f1 = f1_score(y_test_fraud, y_pred_thresh)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "\n",
        "y_pred_phase2 = (y_pred_proba_baseline >= best_threshold).astype(int)\n",
        "metrics_phase2 = plot_confusion_matrix_detailed(\n",
        "    y_test_fraud, y_pred_phase2,\n",
        "    f\"Phase 2: Optimized Threshold ({best_threshold:.2f})\"\n",
        ")\n",
        "\n",
        "improvement_phase2 = ((metrics_phase2['f1'] - baseline_f1) / baseline_f1) * 100\n",
        "print(f\"\\nðŸ“ˆ Improvement: +{improvement_phase2:.1f}% in F1-Score\")\n",
        "print(f\"   Strategy: Adjusted threshold from 0.5 to {best_threshold:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcTrbj-vnbbw"
      },
      "outputs": [],
      "source": [
        "# Phase 3: Handle Class Imbalance with SMOTE\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"# PHASE 3: CLASS IMBALANCE HANDLING (SMOTE)\")\n",
        "print(\"#\"*80)\n",
        "\n",
        "# Already balanced earlier, use those results\n",
        "phase3_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "phase3_model.fit(X_train_fraud_balanced, y_train_fraud_balanced)\n",
        "y_pred_proba_phase3 = phase3_model.predict_proba(X_test_fraud)[:, 1]\n",
        "y_pred_phase3 = (y_pred_proba_phase3 >= best_threshold).astype(int)\n",
        "\n",
        "metrics_phase3 = plot_confusion_matrix_detailed(\n",
        "    y_test_fraud, y_pred_phase3,\n",
        "    \"Phase 3: SMOTE + Optimized Threshold\"\n",
        ")\n",
        "\n",
        "improvement_phase3 = ((metrics_phase3['f1'] - baseline_f1) / baseline_f1) * 100\n",
        "print(f\"\\nðŸ“ˆ Cumulative Improvement: +{improvement_phase3:.1f}% in F1-Score\")\n",
        "print(f\"   Strategy: SMOTE oversampling + threshold optimization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQjw0zXinbbw"
      },
      "outputs": [],
      "source": [
        "# Phase 4: Advanced Model (Gradient Boosting)\n",
        "print(\"\\n\" + \"#\"*80)\n",
        "print(\"# PHASE 4: ADVANCED MODEL (Gradient Boosting)\")\n",
        "print(\"#\"*80)\n",
        "\n",
        "# Use the already trained gradient boosting model\n",
        "y_pred_proba_phase4 = model_fraud_balanced.predict_proba(X_test_fraud)[:, 1]\n",
        "y_pred_phase4 = (y_pred_proba_phase4 >= best_threshold).astype(int)\n",
        "\n",
        "metrics_phase4 = plot_confusion_matrix_detailed(\n",
        "    y_test_fraud, y_pred_phase4,\n",
        "    \"Phase 4: Gradient Boosting + SMOTE + Optimized Threshold\"\n",
        ")\n",
        "\n",
        "improvement_phase4 = ((metrics_phase4['f1'] - baseline_f1) / baseline_f1) * 100\n",
        "print(f\"\\nðŸ“ˆ Total Improvement: +{improvement_phase4:.1f}% in F1-Score\")\n",
        "print(f\"   Strategy: Advanced algorithm + SMOTE + threshold tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsQgW83mnbbw"
      },
      "outputs": [],
      "source": [
        "# Visualize milestone progression\n",
        "phases = ['Phase 1\\nBaseline', 'Phase 2\\nThreshold', 'Phase 3\\nSMOTE', 'Phase 4\\nGBM']\n",
        "f1_scores = [baseline_f1, metrics_phase2['f1'], metrics_phase3['f1'], metrics_phase4['f1']]\n",
        "recalls = [baseline_recall, metrics_phase2['recall'], metrics_phase3['recall'], metrics_phase4['recall']]\n",
        "precisions = [metrics_baseline['precision'], metrics_phase2['precision'],\n",
        "             metrics_phase3['precision'], metrics_phase4['precision']]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Metric progression\n",
        "x = np.arange(len(phases))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax1.bar(x - width, f1_scores, width, label='F1-Score', color='#f39c12', alpha=0.8, edgecolor='black')\n",
        "bars2 = ax1.bar(x, recalls, width, label='Recall', color='#2ecc71', alpha=0.8, edgecolor='black')\n",
        "bars3 = ax1.bar(x + width, precisions, width, label='Precision', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax1.set_ylabel('Score', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Improvement Phase', fontsize=14, fontweight='bold')\n",
        "ax1.set_title('Milestone-Based Performance Improvement', fontsize=16, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(phases, fontsize=11)\n",
        "ax1.legend(fontsize=12)\n",
        "ax1.set_ylim([0, 1.0])\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels\n",
        "for bars in [bars1, bars2, bars3]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.3f}',\n",
        "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Plot 2: Cumulative improvement\n",
        "improvements = [(f1 - baseline_f1) / baseline_f1 * 100 for f1 in f1_scores]\n",
        "colors_improvement = ['#95a5a6', '#3498db', '#f39c12', '#2ecc71']\n",
        "\n",
        "bars_imp = ax2.bar(phases, improvements, color=colors_improvement, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax2.set_ylabel('F1-Score Improvement (%)', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Improvement Phase', fontsize=14, fontweight='bold')\n",
        "ax2.set_title('Cumulative F1-Score Improvement', fontsize=16, fontweight='bold')\n",
        "ax2.axhline(y=0, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "for bar, imp in zip(bars_imp, improvements):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "            f'+{imp:.1f}%',\n",
        "            ha='center', va='bottom', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MILESTONE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Phase 1 (Baseline):          F1 = {baseline_f1:.3f}\")\n",
        "print(f\"Phase 2 (+ Threshold):       F1 = {metrics_phase2['f1']:.3f} (+{improvements[1]:.1f}%)\")\n",
        "print(f\"Phase 3 (+ SMOTE):           F1 = {metrics_phase3['f1']:.3f} (+{improvements[2]:.1f}%)\")\n",
        "print(f\"Phase 4 (+ GBM):             F1 = {metrics_phase4['f1']:.3f} (+{improvements[3]:.1f}%)\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nðŸŽ¯ KEY LESSONS:\")\n",
        "print(\"   1. Start simple (baseline) to establish metrics\")\n",
        "print(\"   2. Optimize threshold first (low-cost, high-impact)\")\n",
        "print(\"   3. Address class imbalance (SMOTE, class weights)\")\n",
        "print(\"   4. Try advanced algorithms (ensemble methods)\")\n",
        "print(\"   5. Iterate and measure improvement at each step\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icVsL1nfnbbw"
      },
      "source": [
        "---\n",
        "## 9. Final Summary and Best Practices\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **No Single Metric is Perfect**\n",
        "   - Always consider multiple metrics\n",
        "   - Context and business requirements matter\n",
        "\n",
        "2. **Precision-Recall Trade-off**\n",
        "   - Increasing Precision typically decreases Recall\n",
        "   - Increasing Recall typically decreases Precision\n",
        "   - F1-Score finds the balance\n",
        "\n",
        "3. **Dataset Balance Matters**\n",
        "   - Balanced: Accuracy is reliable\n",
        "   - Imbalanced: Accuracy is misleading, use Recall/Precision/F1/PR-AUC\n",
        "\n",
        "4. **Domain-Specific Choices**\n",
        "   - Medical: Prioritize Recall (don't miss diseases)\n",
        "   - Spam: Prioritize Precision (don't block important emails)\n",
        "   - Fraud: Balance with F1-Score\n",
        "\n",
        "5. **ROC vs PR Curves**\n",
        "   - ROC-AUC: Good for balanced datasets\n",
        "   - PR-AUC: Better for imbalanced datasets\n",
        "\n",
        "6. **Improvement Strategy**\n",
        "   - Start with baseline\n",
        "   - Optimize threshold\n",
        "   - Handle class imbalance\n",
        "   - Try advanced algorithms\n",
        "   - Measure at each milestone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxiCLjE0nbbw"
      },
      "outputs": [],
      "source": [
        "# Create final summary visualization\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFUSION MATRIX & MODEL EVALUATION - COMPLETE GUIDE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nâœ… Notebook completed successfully!\")\n",
        "print(\"\\nYou've learned:\")\n",
        "print(\"   â€¢ How to interpret confusion matrices\")\n",
        "print(\"   â€¢ Trade-offs between Precision, Recall, F1, and Accuracy\")\n",
        "print(\"   â€¢ Real-world case studies (Medical, Fraud, Spam)\")\n",
        "print(\"   â€¢ ROC and AUC analysis\")\n",
        "print(\"   â€¢ Metric selection for balanced vs imbalanced datasets\")\n",
        "print(\"   â€¢ Milestone-based improvement strategies\")\n",
        "print(\"\\nðŸŽ¯ Next Steps:\")\n",
        "print(\"   1. Apply these concepts to your own datasets\")\n",
        "print(\"   2. Experiment with different thresholds\")\n",
        "print(\"   3. Try various class balancing techniques\")\n",
        "print(\"   4. Always validate with domain experts\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}